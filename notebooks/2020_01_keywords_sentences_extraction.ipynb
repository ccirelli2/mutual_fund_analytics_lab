{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps:\n",
    "    step 1: clean up columns by addressing abbreviations\n",
    "    Step 2: crate dataframe that contain all public health sentences\n",
    "    Step 3: for each row, extract sentences that contain at least one keywords that blong to public health keyword lists for each sentences, put a new row into the dataframe, \n",
    "    step 4: save the new pandas dataframe into excel file\n",
    "    Step 5: repeat step 2, 3, and 4 for natural disaster keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"df2010_2020_ph_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98979, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>accession#</th>\n",
       "      <th>filing_year</th>\n",
       "      <th>fund_name</th>\n",
       "      <th>principal_risks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000004568-10-000004</td>\n",
       "      <td>2010</td>\n",
       "      <td>american balanced fund inc</td>\n",
       "      <td>you may lose money by investing in the fund. t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0001193125-10-068753</td>\n",
       "      <td>2010</td>\n",
       "      <td>massmutual premier enhanced index growth fund</td>\n",
       "      <td>the following are the principal risks of the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0001193125-10-068764</td>\n",
       "      <td>2010</td>\n",
       "      <td>massmutual premier discovery value fund</td>\n",
       "      <td>the following are the principal risks of the f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            accession#  filing_year  \\\n",
       "0           0  0000004568-10-000004         2010   \n",
       "1           1  0001193125-10-068753         2010   \n",
       "2           2  0001193125-10-068764         2010   \n",
       "\n",
       "                                       fund_name  \\\n",
       "0                     american balanced fund inc   \n",
       "1  massmutual premier enhanced index growth fund   \n",
       "2        massmutual premier discovery value fund   \n",
       "\n",
       "                                     principal_risks  \n",
       "0  you may lose money by investing in the fund. t...  \n",
       "1  the following are the principal risks of the f...  \n",
       "2  the following are the principal risks of the f...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019    10753\n",
       "2020    10682\n",
       "2018    10184\n",
       "2017     9990\n",
       "2016     9803\n",
       "2015     9486\n",
       "2014     9017\n",
       "2013     8531\n",
       "2012     7854\n",
       "2011     7207\n",
       "2010     5472\n",
       "Name: filing_year, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['filing_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.dropna(subset=['principal_risks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020    10592\n",
       "2019    10451\n",
       "2018    10184\n",
       "2017     9990\n",
       "2016     9803\n",
       "2015     9486\n",
       "2014     9017\n",
       "2013     8531\n",
       "2012     7854\n",
       "2011     7207\n",
       "2010     5472\n",
       "Name: filing_year, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['filing_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {'dr.': 'doctor', 'mr.': 'mister', 'bro.': 'brother', 'bro': 'brother', 'mrs.': 'mistress', 'ms.': 'miss', 'jr.': 'junior', 'sr.': 'senior',\n",
    "                 'e.g.': 'for example', 'vs.': 'versus', 'U.S.': 'United States','etc.': 'etcetera', 'J.P.': 'Justice of the Peace',\n",
    "                 'Inc.': 'Incorporated', 'LLC.': 'limited liability corporation', 'Co.': 'company', 'l.p.': 'limited partneship',\n",
    "                 'ltd.': 'limited', 'Jan.': 'January', 'Feb.': 'February', 'Mar.': 'March', 'Apr.': 'April', 'i.e.': 'for example',\n",
    "                 'Jun.': 'June', 'Jul.': 'July', 'Aug.': 'August', 'Oct.': 'October', 'Dec.': 'December', 'S.E.C.': 'SEC', 'Inv. Co. Act': 'Investment Company Act'}\n",
    "terminators = ['.', '!', '?']\n",
    "wrappers = ['\"', \"'\", ')', ']', '}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(a_str, sub):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.find(sub, start)\n",
    "        if start == -1:\n",
    "            return\n",
    "        yield start\n",
    "        start += len(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentence_end(paragraph):\n",
    "    [possible_endings, contraction_locations] = [[], []]\n",
    "    contractions = abbreviations.keys()\n",
    "    sentence_terminators = terminators + [terminator + wrapper for wrapper in wrappers for terminator in terminators]\n",
    "    for sentence_terminator in sentence_terminators:\n",
    "        t_indices = list(find_all(paragraph, sentence_terminator))\n",
    "        possible_endings.extend(([] if not len(t_indices) else [[i, len(sentence_terminator)] for i in t_indices]))\n",
    "    for contraction in contractions:\n",
    "        c_indices = list(find_all(paragraph, contraction))\n",
    "        contraction_locations.extend(([] if not len(c_indices) else [i + len(contraction) for i in c_indices]))\n",
    "    possible_endings = [pe for pe in possible_endings if pe[0] + pe[1] not in contraction_locations]\n",
    "    if len(paragraph) in [pe[0] + pe[1] for pe in possible_endings]:\n",
    "        max_end_start = max([pe[0] for pe in possible_endings])\n",
    "        possible_endings = [pe for pe in possible_endings if pe[0] != max_end_start]\n",
    "    possible_endings = [pe[0] + pe[1] for pe in possible_endings if sum(pe) > len(paragraph) or (sum(pe) < len(paragraph) and paragraph[sum(pe)] == ' ')]\n",
    "    end = (-1 if not len(possible_endings) else max(possible_endings))\n",
    "    return end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences(paragraph):\n",
    "   replacingList = [[\"?\", \"? \"], [\"!\", \"! \"],[\".\", \". \"],[\"(\", \" \"], [\")\", \" \"], [\",\", \" \"], [\"i. e.\", \"i.e\"], [\"e. g.\", \"e.g.\"],[\"U. S.\", \"U.S.\"], [\"J. P.\", \"J.P.\"], [\"l. p.\", \"l.p.\"], [\"S. E. C.\", \"S.E.C.\"]]\n",
    "   for items in replacingList:\n",
    "       paragraph = paragraph.replace(items[0], items[1])\n",
    "   paragraph = re.sub(' +', ' ', paragraph)\n",
    "   if paragraph != \"\":\n",
    "       if paragraph[0] == '?' or paragraph[0] == '.' or paragraph[0] == '!':\n",
    "           paragraph = paragraph[1:]\n",
    "   end = True\n",
    "   sentences = []\n",
    "   while end > -1:\n",
    "       end = find_sentence_end(paragraph)\n",
    "       if end > -1:\n",
    "           sentences.append(paragraph[end:].strip())\n",
    "           paragraph = paragraph[:end]\n",
    "   sentences.append(paragraph)\n",
    "   sentences.reverse()\n",
    "   return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#input paragraphs, then split into sentences by using the above function find_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-1cbc1285c4e4>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data1['cleaned_principal_risks']= \"\"\n",
      "<ipython-input-13-1cbc1285c4e4>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data1['cleaned_principal_risks'].iloc[row] = sents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing row:  0\n",
      "processing row:  5000\n",
      "processing row:  10000\n",
      "processing row:  15000\n",
      "processing row:  20000\n",
      "processing row:  25000\n",
      "processing row:  30000\n",
      "processing row:  35000\n",
      "processing row:  40000\n",
      "processing row:  45000\n",
      "processing row:  50000\n",
      "processing row:  55000\n",
      "processing row:  60000\n",
      "processing row:  65000\n",
      "processing row:  70000\n",
      "processing row:  75000\n",
      "processing row:  80000\n",
      "processing row:  85000\n",
      "processing row:  90000\n",
      "processing row:  95000\n"
     ]
    }
   ],
   "source": [
    "data1['cleaned_principal_risks']= \"\"\n",
    "for row in range(len(data1)):\n",
    "    if row %5000 == 0:\n",
    "        print (\"processing row: \", row)\n",
    "    para = data1['principal_risks'].iloc[row]\n",
    "    sents = find_sentences(para)\n",
    "    data1['cleaned_principal_risks'].iloc[row] = sents\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [you may lose money by investing in the fund.,...\n",
       "1    [the following are the principal risks of the ...\n",
       "2    [the following are the principal risks of the ...\n",
       "3    [it is important to note that this fund seeks ...\n",
       "4    [the following are the principal risks of the ...\n",
       "Name: cleaned_principal_risks, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['cleaned_principal_risks'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv('2020_01_principal_risks_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Health Keywords to Search Sentences that Contain one or multiple of these keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Public Health keywords: \n",
    "ph_word = ['communicable diseases','health crises','pandemic','respiratory','illness','prevention','epidemic',\n",
    "           'coronavirus','health crisis','pandemics','sanitation','global health crises',\n",
    "           'covid','health screenings','pathogens','sars','global health crisis',\n",
    "           'covid 19','hiv','preparedness','sars cov 2','epidemics','disease','influenza',\n",
    "           'public health','virus','global health','diseases','mers','quarantines','h1n1','viruses']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'tokenizer' from 'nltk.tokenize' (C:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-9165ba7c5bef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mpubic_health\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'tokenizer' from 'nltk.tokenize' (C:\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk.data\n",
    "import re\n",
    "from nltk.tokenize import tokenizer\n",
    "\n",
    "pubic_health = pd.DataFrame()\n",
    "\n",
    "for row in range(10): #range(data1):\n",
    "    if row %5000 == 0:\n",
    "        print (\"processing row: \", row)\n",
    "    sentences = tokenizer.tokenize(data1['cleaned_principal_risks'].iloc[row])\n",
    "    for i in range(len(sentences)):\n",
    "        for word in ph_word:\n",
    "            tokens = sentences[i].lower().split()\n",
    "            if word in tokens:\n",
    "                pubic_health = pubic_health.append({'accession#':data1['accession#'].iloc[row],\n",
    "                                                    'filing_year':data1['filing_year'].iloc[row],\n",
    "                                                    'fund_name':data1['fund_name'].iloc[row],                                                    \n",
    "                                                    'sentences':re.sub(' +', ' ',\" \".join(sentences[i]).replace('\\n',' '))},ignore_index=True)\n",
    "                   \n",
    "                break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubic_health.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubic_health.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubic_health.to_excel(\"2020_01_public_health_keywords_sentences.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Disaster Keywords to Search Sentences that Contain one or multiple of these keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Disaster keywords:\n",
    "nd_word = ['cloud','fire','natural disaster','tornadoes','earthquake',\n",
    "           'cyclones','flood','natural disasters','tsunami','disaster', \n",
    "           'death','floods','seismic','tsunamis','disasters',\n",
    "           'drought','hurricane','storms','underground',\n",
    "           'earthquakes','hurricanes','windstorms','volcano'\n",
    "           'environmental','damage','lightning','droughts','volcanoes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each set of keywords, remove duplicate setences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the datasets into excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics of the above three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample code to extract setences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = ['public health','natural disaster','911']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract one sentences before and one sentences after the focus sentenceÂ¶ that contains one of the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk.data\n",
    "import re\n",
    "df2 = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    for f in glob.glob(os.path.join(path,filename, '*.txt')):\n",
    "        file = open(f,\"r\")\n",
    "        data = file.read()\n",
    "        sentences = tokenizer.tokenize(data)\n",
    "        for i in range(len(sentences)):\n",
    "            for word in search_words:\n",
    "                tokens = sentences[i].lower().split()\n",
    "                if word in tokens:\n",
    "                    if i > 0 and i < len(sentences)-1:\n",
    "                        df2 = df2.append({'id':f,'sentences':re.sub(' +', ' ',\" \".join(sentences[i-1:i+2]).replace('\\n',' '))},ignore_index=True)\n",
    "                    elif i == 0:\n",
    "                        df2 = df2.append({'id':f,'sentences':re.sub(' +', ' ',\" \".join(sentences[0:i+2]).replace('\\n',' '))},ignore_index=True)\n",
    "                    else:\n",
    "                        df2 = df2.append({'id':f,'sentences':re.sub(' +', ' ',\" \".join(sentences[i-1:len(sentences)]).replace('\\n',' '))},ignore_index=True)\n",
    "                    break   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
